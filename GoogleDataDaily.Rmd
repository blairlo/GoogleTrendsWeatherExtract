---
title: "GoogleDataCOD"
author: "Logan Blair"
date: "September 21, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Prelims
```{r message=FALSE, warning=FALSE}
Packages <- c("digest", 
              "glue", 
              "reticulate", 
              "knitr",
              "maps",
              "ggplot2",
              "viridis",
              "pals",
              "scico",
              "ggrepel",
              "tidyr",
              "tidyverse",
              "rdrop2",
              "fansi",
              "lubridate",
              "ExcelFunctionsR",
              "gtrendsR"
              )

lapply(Packages, library, character.only = TRUE)


```

Load Various Lists and Crosswalks
```{r}
GoogleFIPS<-read.csv("StateGoogleCrosswalk.csv")
DMA<-read.csv("google_metro_areas.csv")
DMAFIPS<-read.csv("GoogleTrends_CountyDMA_Mapping.csv")
```


From Google:
Values are calculated on a scale from 0 to 100, where 100 is the location with the most popularity as a fraction of total searches in that location, a value of 50 indicates a location which is half as popular. A value of 0 indicates a location where there was not enough data for this term. Note: A higher value means a higher proportion of all queries, not a higher absolute query count. So a tiny country where 80% of the queries are for "bananas" will get twice the score of a giant country where only 40% of the queries are for "bananas".



#Challenge 1: Monthly to Daily Frequency 
Google trends only gives daily data for searched <= 1 month, Weekly <=9months and monthly > 9 months. Aley Dyachenko came up with a clever work around function which gives daily data, and preserves the index: http://alexdyachenko.com/all/how-to-get-daily-google-trends-data-for-any-period-with-r/ . Ive made a couple adjustment to generalize the function further, and allow for multiple geographies.

```{r}
get_daily_gtrend <- function(geo, keyword, from , to,low_search_volume=TRUE) { #generalize the function
  if (ymd(to) >= floor_date(Sys.Date(), 'month')) {
    to <- floor_date(ymd(to), 'month') - days(1)
    
    if (to < from) {
      stop("Specifying \'to\' date in the current month is not allowed")
    }
  }
  
  aggregated_data <- gtrends( geo = geo,keyword = keyword, time = paste(from, to),low_search_volume=TRUE)
  if(is.null(aggregated_data$interest_over_time)) {
    print('There is no data in Google Trends!')
    return()
  }

  mult_m <- aggregated_data$interest_over_time %>%
    mutate(hits = as.integer(ifelse(hits == '<1', '0', hits))) %>%
    group_by(month = floor_date(date, 'month'), keyword) %>%
    summarise(hits = sum(hits)) %>%
    ungroup() %>%
    mutate(ym = format(month, '%Y-%m'),
           mult = hits / max(hits)) %>%
    select(month, ym, keyword, mult) %>%
    as_tibble()
  
  pm <- tibble(s = seq(ymd(from), ymd(to), by = 'month'), 
               e = seq(ymd(from), ymd(to), by = 'month') + months(1) - days(1))
  
  raw_trends_m <- tibble()
  
  for (i in seq(1, nrow(pm), 1)) {
    curr <- gtrends(geo = geo, keyword, time = paste(pm$s[i], pm$e[i]),low_search_volume=TRUE)
    if(is.null(curr$interest_over_time)) next
    print(paste('for', pm$s[i], pm$e[i], 'retrieved', count(curr$interest_over_time), 'days of data (all keywords)'))
    raw_trends_m <- rbind(raw_trends_m,
                         curr$interest_over_time)
  }
  
  trend_m <- raw_trends_m %>%
    select(date, keyword, geo, hits) %>% #add geo here to account for multiple geographies
    mutate(ym = format(date, '%Y-%m'),
           hits = as.integer(ifelse(hits == '<1', '0', hits))) %>%
    as_tibble()
  
  trend_res <- trend_m %>%
    left_join(mult_m) %>%
    mutate(est_hits = hits * mult) %>%
    select(date, keyword, geo, est_hits) %>% #add geo here to account for multiple geographies
    as_tibble() %>%
    mutate(date = as.Date(date))
  Sys.sleep(60)
  
  return(trend_res)
}

```



```{r}

get_daily_gtrend <- function(geo ,keyword , from , to,low_search_volume=TRUE) {
  if (ymd(to) >= floor_date(Sys.Date(), 'month')) {
    to <- floor_date(ymd(to), 'month') - days(1)
    
    if (to < from) {
      stop("Specifying \'to\' date in the current month is not allowed")
    }
  }
  
  aggregated_data <- gtrends(geo = geo, keyword = keyword,  time = paste(from, to),low_search_volume=TRUE)
  if(is.null(aggregated_data$interest_over_time)) {
    print('There is no data in Google Trends!')
    return()
  }

  mult_m <- aggregated_data$interest_over_time %>%
    mutate(hits = as.integer(ifelse(hits == '<1', '0', hits))) %>%
    group_by(month = floor_date(date, 'month'), keyword) %>%
    summarise(hits = sum(hits)) %>%
    ungroup() %>%
    mutate(ym = format(month, '%Y-%m'),
           mult = hits / max(hits)) %>%
    select(month, ym, keyword, mult) %>%
    as_tibble()
  
  pm <- tibble(s = seq(ymd(from), ymd(to), by = 'month'), 
               e = seq(ymd(from), ymd(to), by = 'month') + months(1) - days(1))
  
  raw_trends_m <- tibble()
  
  for (i in seq(1, nrow(pm), 1)) {
    curr <- gtrends(geo = geo,keyword,  time = paste(pm$s[i], pm$e[i]),low_search_volume=TRUE)
    if(is.null(curr$interest_over_time)) next
    print(paste('for', pm$s[i], pm$e[i], 'retrieved', count(curr$interest_over_time), 'days of data (all keywords)'))
    raw_trends_m <- rbind(raw_trends_m,
                         curr$interest_over_time)
  }
  
  trend_m <- raw_trends_m %>%
    select(date, keyword, hits) %>%
    mutate(ym = format(date, '%Y-%m'),
           hits = as.integer(ifelse(hits == '<1', '0', hits))) %>%
    as_tibble()
  
  trend_res <- trend_m %>%
    left_join(mult_m) %>%
    mutate(est_hits = hits * mult) %>%
    select(date, keyword, est_hits) %>%
    as_tibble() %>%
    mutate(date = as.Date(date))
  
  Sys.sleep(120)
  return(trend_res)
}

```

For Testing and Comparison
```{r}
#Set Parameters
keyword<-'weather'
from<-'2013-01-01'
to<-'2013-03-31'
baseline<-"US-NY-501"

#compile query groups

Group1 <- get_daily_gtrend( geo ='US-NY-501', keyword = c('weather'), from='2013-01-01',to ='2013-03-31')
 

test<-ggplot(Group1, aes(x = date, y = est_hits)) + 
  geom_line(aes(color = keyword)) #plot
                        
ggplot(Group1, aes(x = date, y = est_hits)) + 
  geom_ma(aes(color = geo),ma_fun = SMA, n = 10) #plot 30 day moving average


#Set Parameters
search<-"weather"
time<-"2013-01-01 2013-03-31"
baseline<-"US-CA-771"


Group2 <- gtrends(c('weather'), time = time, geo = c("US-NY-501"))


test
plot(Group2) #plot monthly data
                      
```

#Challenge 2: Google will only allow groups of 5 indexed to be searched.

An additional issue is that each search is indexed to the highest search volume in that query, making the groups not comparable across query groups. Some have suggested finding and keeping a search term that always has the highest search volume, but does not overwhelm others in the index (or index vlaues can be very small and loose variation). In other words we need something that gets searched close to "weather" search volume but always just a little more popular.

https://towardsdatascience.com/using-google-trends-at-scale-1c8b902b6bfa


Scaling up: Make a query list to iterate over

#Daily Index for one term relative to itself in each DMA
```{r message=FALSE}
#Set Parameters
keyword<-c('weather','dog')
from<-'2015-01-01'
to<-'2021-12-31'

DMA.list <- setNames(split(DMA, seq(nrow(DMA))), rownames(DMA)) #make a list of all (210 DMAs)
DMA.list <- lapply(DMA.list, paste, collapse = " ") #make list text strings
DMA_list2<-lapply(DMA.list,get_daily_gtrend, keyword=keyword, from=from, to=to) #loop over list to request trends for each DMA
DMA_DF2<-bind_rows(DMA_list2, .id = "column_label") #stack results into 1 data frame

```


#Daily Index for one term relative to itself in each DMA
```{r}
#Set Parameters
keyword<-'weather'
reference<-'furnature'
from<-'2005-01-01'
to<-'2014-12-31'

DMA.list <- setNames(split(DMA, seq(nrow(DMA))), rownames(DMA))
DMA.list <- lapply(DMA.list, paste, collapse = " ")
DMA_list2<-lapply(DMA.list,get_daily_gtrend, keyword=c(keyword,reference), from=from, to=to)#
DMA_DF2<-bind_rows(DMA_list2, .id = "column_label")

```


#Set Index Relative to highest Search Volume DMA in the US
```{r}
baseline<-"US-NY-501"

QueryList<-list(
Group1Dat=c(baseline,"US-OR-813", "US-AR-670", "US-OH-515", "US-PA-574"),
Group2Dat=c(baseline,"US-OH-554")
)

```

Set Parameters and run
```{r message=FALSE}
#Set Parameters
keyword<-c('weather')
from<-'2013-01-01'
to<-'2014-12-31'


DMA_list1<-lapply(QueryList,get_daily_gtrend, keyword=keyword, from=from, to=to)#
DMA_DF1<-bind_rows(DMA_list1, .id = "column_label")


#Some plots
ggplot(Trends_DMA_DF, aes(x = date, y = est_hits)) + 
  geom_line(aes(color = geo)) #daily
                        
ggplot(Trends_DMA_DF, aes(x = date, y = est_hits)) + 
  geom_ma(aes(color = geo),ma_fun = SMA, n = 10) #30 day moving average
 
```


Send to Dropbox
```{r}
#drop_upload('GoogleClimateChange.csv', path = "Cost of Denial/Data/Google")
#drop_upload('GoogleGlobalWarming.csv', path = "Cost of Denial/Data/Google")
#drop_upload('GoogleClimateHoax.csv', path = "Cost of Denial/Data/Google")
#drop_upload('GooglePuppies.csv', path = "Cost of Denial/Data/Google")
```


Map the last group of API calls
```{r}
search <- gtrends("puppies", 
                      time = time, 
                      gprop = type,
                      low_search_volume = TRUE,
                      geo = c("US"
))


SearchInterestByRegion <- as_tibble(search$interest_by_region)
SearchInterestByRegion <- SearchInterestByRegion %>% 
  dplyr::mutate(region = stringr::str_to_lower(location))

statesMap = ggplot2::map_data("state")

#SearchMerged <- merge(statesMap, Search, by = "region")
SearchMerged <- SearchInterestByRegion %>% dplyr::left_join(x = ., 
                                                            y = statesMap, 
                                                            by = "region")


my_theme <- function() {
    theme_bw() +
    theme(panel.background = element_blank()) +
    theme(plot.background = element_rect(fill = "seashell")) +
    theme(panel.border = element_blank()) +                     # facet border
    theme(strip.background = element_blank()) +                 # facet title background
    theme(plot.margin = unit(c(.5, .5, .5, .5), "cm")) +
    theme(panel.spacing = unit(3, "lines")) +
    theme(panel.grid.major = element_blank()) +
    theme(panel.grid.minor = element_blank()) +
    theme(legend.background = element_blank()) +
    theme(legend.key = element_blank()) +
    theme(legend.title = element_blank())
  }


my_theme2 = function() {
  my_theme() +
    theme(axis.title = element_blank()) +
    theme(axis.text = element_blank()) +
    theme(axis.ticks = element_blank())
}

SearchMerged %>% 
  ggplot(aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, 
                   fill = hits), 
               colour = "white") +
  scale_fill_gradientn(colours = rev(scico(15, 
                                           palette = "tokyo")[2:7])) +
  my_theme2() +
  ggtitle("Google search interest  \nin each state 2015-12-01 to 2015-12-31") 



```


